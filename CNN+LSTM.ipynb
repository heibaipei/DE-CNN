{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import interpolate\n",
    "from ulity import *\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels 3330\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14.]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(111)\n",
    "\n",
    "conv_1_shape = '3*3*1*32'\n",
    "pool_1_shape = 'None'\n",
    "\n",
    "conv_2_shape = '3*3*1*64'\n",
    "pool_2_shape = 'None'\n",
    "\n",
    "conv_3_shape = '3*3*1*128'\n",
    "pool_3_shape = 'None'\n",
    "\n",
    "conv_4_shape = 'None'\n",
    "pool_4_shape = 'None'\n",
    "\n",
    "\n",
    "window_size = 200*5\n",
    "n_lstm_layers = 2\n",
    "# full connected parameter\n",
    "fc_size = 1024\n",
    "n_fc_in = 1024\n",
    "n_fc_out = 1024\n",
    "\n",
    "dropout_prob = 0.5\n",
    "\n",
    "calibration = 'N'\n",
    "norm_type='2D'\n",
    "regularization_method = 'dropout'\n",
    "enable_penalty = False\n",
    "\n",
    "output_dir \t= \"roc1\"\n",
    "output_file = \"roc2\"\n",
    "\n",
    "dataset_dir = \"./result/\"\n",
    "\n",
    "with open(dataset_dir+\"D3_data3_neg2.pkl\", \"rb\") as fp:\n",
    "  \tdatasets = pickle.load(fp)\n",
    "with open(dataset_dir+\"D3_label3_neg2.pkl\", \"rb\") as fp:\n",
    "  \tlabels = pickle.load(fp)\n",
    "\n",
    "datasets = datasets.reshape(len(datasets), window_size, 10, 11, 1)\n",
    "print('labels', len(labels))\n",
    "one_hot_labels = np.array(list(pd.get_dummies(labels)))\n",
    "print(one_hot_labels)\n",
    "labels = np.asarray(pd.get_dummies(labels), dtype = np.int8)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(datasets, labels, test_size=0.1, stratify = labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0831 16:26:49.665343 140599326492416 deprecation.py:506] From <ipython-input-3-8f0a34f26653>:140: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sample: 2997\n",
      "test sample: 333\n",
      "**********(Sat Aug 31 16:26:49 2019) Load and Split dataset End **********\n",
      "\n",
      "**********(Sat Aug 31 16:26:49 2019) Define parameters and functions Begin: **********\n",
      "\n",
      "**********(Sat Aug 31 16:26:49 2019) Define parameters and functions End **********\n",
      "\n",
      "**********(Sat Aug 31 16:26:49 2019) Define NN structure Begin: **********\n",
      "\n",
      "(?, 10, 11, 32)\n",
      "(?, 10, 11, 64)\n",
      "(?, 10, 11, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0831 16:26:50.070472 140599326492416 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0831 16:26:50.071725 140599326492416 deprecation.py:323] From <ipython-input-3-8f0a34f26653>:148: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0831 16:26:50.075576 140599326492416 deprecation.py:323] From <ipython-input-3-8f0a34f26653>:151: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "W0831 16:26:50.090471 140599326492416 deprecation.py:323] From <ipython-input-3-8f0a34f26653>:156: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0831 16:26:50.249303 140599326492416 deprecation.py:506] From /home/ydwang/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0831 16:26:50.256407 140599326492416 deprecation.py:506] From /home/ydwang/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0831 16:26:50.586873 140599326492416 deprecation.py:323] From <ipython-input-3-8f0a34f26653>:183: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********(Sat Aug 31 16:26:51 2019) Define NN structure End **********\n",
      "\n",
      "**********(Sat Aug 31 16:26:51 2019) Train and Test NN Begin: **********\n",
      "\n",
      "(Sat Aug 31 16:31:57 2019) Epoch:  1  Training Cost:  2.71699083453477 Training Accuracy:  0.06835017078603157\n",
      "(Sat Aug 31 16:32:06 2019) Epoch:  1  Test Cost:  2.7331534949215976 Test Accuracy:  0.06363636627793312 \n",
      "\n",
      "(Sat Aug 31 16:37:07 2019) Epoch:  2  Training Cost:  2.6226687768493036 Training Accuracy:  0.1393939434277891\n",
      "(Sat Aug 31 16:37:17 2019) Epoch:  2  Test Cost:  2.6423285874453457 Test Accuracy:  0.112121216614138 \n",
      "\n",
      "(Sat Aug 31 16:42:16 2019) Epoch:  3  Training Cost:  2.3221269906169235 Training Accuracy:  0.17272727752123215\n",
      "(Sat Aug 31 16:42:25 2019) Epoch:  3  Test Cost:  2.3601999933069404 Test Accuracy:  0.14848485216498375 \n",
      "\n",
      "(Sat Aug 31 16:47:25 2019) Epoch:  4  Training Cost:  2.1066719979950874 Training Accuracy:  0.22323232941856289\n",
      "(Sat Aug 31 16:47:35 2019) Epoch:  4  Test Cost:  2.136072375557639 Test Accuracy:  0.19090909443118356 \n",
      "\n",
      "(Sat Aug 31 16:52:33 2019) Epoch:  5  Training Cost:  2.0289816748012197 Training Accuracy:  0.2599326669417246\n",
      "(Sat Aug 31 16:52:42 2019) Epoch:  5  Test Cost:  2.051854512908242 Test Accuracy:  0.26060606810179626 \n",
      "\n",
      "(Sat Aug 31 16:57:41 2019) Epoch:  6  Training Cost:  1.7571184900071886 Training Accuracy:  0.3144781220741946\n",
      "(Sat Aug 31 16:57:51 2019) Epoch:  6  Test Cost:  1.7943382154811511 Test Accuracy:  0.2909090979532762 \n",
      "\n",
      "(Sat Aug 31 17:02:48 2019) Epoch:  7  Training Cost:  1.6418060244935933 Training Accuracy:  0.37912458708189956\n",
      "(Sat Aug 31 17:02:58 2019) Epoch:  7  Test Cost:  1.648513989015059 Test Accuracy:  0.36060607026923786 \n",
      "\n",
      "(Sat Aug 31 17:07:54 2019) Epoch:  8  Training Cost:  1.4468770683413805 Training Accuracy:  0.4033670092772956\n",
      "(Sat Aug 31 17:08:03 2019) Epoch:  8  Test Cost:  1.4562418785962192 Test Accuracy:  0.4090909172188152 \n",
      "\n",
      "(Sat Aug 31 17:13:03 2019) Epoch:  9  Training Cost:  1.3070297692761277 Training Accuracy:  0.4656565713159966\n",
      "(Sat Aug 31 17:13:12 2019) Epoch:  9  Test Cost:  1.3305090123956853 Test Accuracy:  0.4575757655230435 \n",
      "\n",
      "(Sat Aug 31 17:18:10 2019) Epoch:  10  Training Cost:  1.16089949463353 Training Accuracy:  0.5447811538522894\n",
      "(Sat Aug 31 17:18:19 2019) Epoch:  10  Test Cost:  1.1679480563510547 Test Accuracy:  0.5484848591414365 \n",
      "\n",
      "(Sat Aug 31 17:23:16 2019) Epoch:  11  Training Cost:  0.9415596235882152 Training Accuracy:  0.6326599361920597\n",
      "(Sat Aug 31 17:23:26 2019) Epoch:  11  Test Cost:  0.9142006256363608 Test Accuracy:  0.6151515218344602 \n",
      "\n",
      "(Sat Aug 31 17:28:24 2019) Epoch:  12  Training Cost:  0.7602358442364316 Training Accuracy:  0.7202020203224336\n",
      "(Sat Aug 31 17:28:33 2019) Epoch:  12  Test Cost:  0.7373179766264829 Test Accuracy:  0.7181818160143766 \n",
      "\n",
      "(Sat Aug 31 17:33:30 2019) Epoch:  13  Training Cost:  0.7344718638694647 Training Accuracy:  0.7494949475683347\n",
      "(Sat Aug 31 17:33:40 2019) Epoch:  13  Test Cost:  0.7453497864983298 Test Accuracy:  0.766666667027907 \n",
      "\n",
      "(Sat Aug 31 17:38:37 2019) Epoch:  14  Training Cost:  0.3770828566165886 Training Accuracy:  0.8707070657701204\n",
      "(Sat Aug 31 17:38:47 2019) Epoch:  14  Test Cost:  0.3068316009911624 Test Accuracy:  0.8939393812959845 \n",
      "\n",
      "(Sat Aug 31 17:43:44 2019) Epoch:  15  Training Cost:  0.6983022075710874 Training Accuracy:  0.7040404051241248\n",
      "(Sat Aug 31 17:43:53 2019) Epoch:  15  Test Cost:  0.7068473534150557 Test Accuracy:  0.7030303098938682 \n",
      "\n",
      "(Sat Aug 31 17:48:52 2019) Epoch:  16  Training Cost:  0.21472571005649638 Training Accuracy:  0.9272727165559326\n",
      "(Sat Aug 31 17:49:01 2019) Epoch:  16  Test Cost:  0.14860245179046283 Test Accuracy:  0.9515151435678656 \n",
      "\n",
      "(Sat Aug 31 17:53:58 2019) Epoch:  17  Training Cost:  0.1715171313421293 Training Accuracy:  0.9545454455144478\n",
      "(Sat Aug 31 17:54:08 2019) Epoch:  17  Test Cost:  0.19480778276920319 Test Accuracy:  0.9484848271716725 \n",
      "\n",
      "(Sat Aug 31 17:59:04 2019) Epoch:  18  Training Cost:  0.1577860312630432 Training Accuracy:  0.9508417417304684\n",
      "(Sat Aug 31 17:59:13 2019) Epoch:  18  Test Cost:  0.1379652742973783 Test Accuracy:  0.9606060494076122 \n",
      "\n",
      "(Sat Aug 31 18:04:11 2019) Epoch:  19  Training Cost:  0.13895030078863857 Training Accuracy:  0.9471380337320193\n",
      "(Sat Aug 31 18:04:20 2019) Epoch:  19  Test Cost:  0.12802463431249966 Test Accuracy:  0.9484848325902765 \n",
      "\n",
      "(Sat Aug 31 18:09:16 2019) Epoch:  20  Training Cost:  0.10964778801332219 Training Accuracy:  0.9609427500252772\n",
      "(Sat Aug 31 18:09:26 2019) Epoch:  20  Test Cost:  0.09234891865741122 Test Accuracy:  0.966666655106978 \n",
      "\n",
      "(Sat Aug 31 18:14:23 2019) Epoch:  21  Training Cost:  0.07259033920450343 Training Accuracy:  0.9710437601262872\n",
      "(Sat Aug 31 18:14:32 2019) Epoch:  21  Test Cost:  0.06067262069237503 Test Accuracy:  0.9818181720646945 \n",
      "\n",
      "(Sat Aug 31 18:19:29 2019) Epoch:  22  Training Cost:  0.09253978759821768 Training Accuracy:  0.9670033539184416\n",
      "(Sat Aug 31 18:19:39 2019) Epoch:  22  Test Cost:  0.07475015700964088 Test Accuracy:  0.9727272662249479 \n",
      "\n",
      "(Sat Aug 31 18:24:36 2019) Epoch:  23  Training Cost:  0.2278854620403074 Training Accuracy:  0.9427609323251127\n",
      "(Sat Aug 31 18:24:45 2019) Epoch:  23  Test Cost:  0.2934868223965168 Test Accuracy:  0.9303030209107832 \n",
      "\n",
      "(Sat Aug 31 18:29:43 2019) Epoch:  24  Training Cost:  0.08731627136450072 Training Accuracy:  0.972053860775148\n",
      "(Sat Aug 31 18:29:52 2019) Epoch:  24  Test Cost:  0.11420324487103657 Test Accuracy:  0.9606060494076122 \n",
      "\n",
      "(Sat Aug 31 18:34:49 2019) Epoch:  25  Training Cost:  0.4610609351045857 Training Accuracy:  0.9020201943137429\n",
      "(Sat Aug 31 18:34:58 2019) Epoch:  25  Test Cost:  0.5491371730511839 Test Accuracy:  0.9060605981133201 \n",
      "\n",
      "(Sat Aug 31 18:39:55 2019) Epoch:  26  Training Cost:  0.13515448539207378 Training Accuracy:  0.9636363519562615\n",
      "(Sat Aug 31 18:40:04 2019) Epoch:  26  Test Cost:  0.20223853720182722 Test Accuracy:  0.9606060494076122 \n",
      "\n",
      "(Sat Aug 31 18:45:02 2019) Epoch:  27  Training Cost:  0.15649128353724143 Training Accuracy:  0.9515151411595971\n",
      "(Sat Aug 31 18:45:11 2019) Epoch:  27  Test Cost:  0.13848675990646536 Test Accuracy:  0.9484848325902765 \n",
      "\n",
      "(Sat Aug 31 18:50:09 2019) Epoch:  28  Training Cost:  0.20314007794550348 Training Accuracy:  0.9528619424261228\n",
      "(Sat Aug 31 18:50:18 2019) Epoch:  28  Test Cost:  0.24225146615539084 Test Accuracy:  0.9515151381492615 \n",
      "\n",
      "(Sat Aug 31 18:55:15 2019) Epoch:  29  Training Cost:  0.27977362969088265 Training Accuracy:  0.9424242305033135\n",
      "(Sat Aug 31 18:55:24 2019) Epoch:  29  Test Cost:  0.16234624572098255 Test Accuracy:  0.9515151381492615 \n",
      "\n",
      "(Sat Aug 31 19:00:22 2019) Epoch:  30  Training Cost:  0.194282680524118 Training Accuracy:  0.9420875316918499\n",
      "(Sat Aug 31 19:00:31 2019) Epoch:  30  Test Cost:  0.19446737657893787 Test Accuracy:  0.9454545324498956 \n",
      "\n",
      "(Sat Aug 31 19:05:28 2019) Epoch:  31  Training Cost:  0.3138955307400061 Training Accuracy:  0.9134680032730103\n",
      "(Sat Aug 31 19:05:37 2019) Epoch:  31  Test Cost:  0.29698941348628566 Test Accuracy:  0.9303030100735751 \n",
      "\n",
      "(Sat Aug 31 19:10:34 2019) Epoch:  32  Training Cost:  0.14532095682542687 Training Accuracy:  0.9565656468121693\n",
      "(Sat Aug 31 19:10:43 2019) Epoch:  32  Test Cost:  0.13873253030363808 Test Accuracy:  0.9666666496883739 \n",
      "\n",
      "(Sat Aug 31 19:15:42 2019) Epoch:  33  Training Cost:  0.03154804786916227 Training Accuracy:  0.9885521814076588\n",
      "(Sat Aug 31 19:15:51 2019) Epoch:  33  Test Cost:  0.06567070838487284 Test Accuracy:  0.9848484722050753 \n",
      "\n",
      "(Sat Aug 31 19:20:49 2019) Epoch:  34  Training Cost:  0.23854858650987018 Training Accuracy:  0.9437710317698392\n",
      "(Sat Aug 31 19:20:58 2019) Epoch:  34  Test Cost:  0.2364913324338638 Test Accuracy:  0.9424242377281189 \n",
      "\n",
      "(Sat Aug 31 19:25:55 2019) Epoch:  35  Training Cost:  0.11914063276160941 Training Accuracy:  0.9649831508145188\n",
      "(Sat Aug 31 19:26:04 2019) Epoch:  35  Test Cost:  0.06473854586312716 Test Accuracy:  0.9848484776236794 \n",
      "\n",
      "(Sat Aug 31 19:31:02 2019) Epoch:  36  Training Cost:  0.4079459809987439 Training Accuracy:  0.9171717070569896\n",
      "(Sat Aug 31 19:31:11 2019) Epoch:  36  Test Cost:  0.35106041350147943 Test Accuracy:  0.9242424152114175 \n",
      "\n",
      "(Sat Aug 31 19:36:08 2019) Epoch:  37  Training Cost:  0.25720559955384076 Training Accuracy:  0.9370370248351434\n",
      "(Sat Aug 31 19:36:17 2019) Epoch:  37  Test Cost:  0.16660064361481505 Test Accuracy:  0.9606060494076122 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Sat Aug 31 19:41:15 2019) Epoch:  38  Training Cost:  0.10873626812385875 Training Accuracy:  0.9629629525271329\n",
      "(Sat Aug 31 19:41:24 2019) Epoch:  38  Test Cost:  0.1364811279768632 Test Accuracy:  0.9606060548262163 \n",
      "\n",
      "(Sat Aug 31 19:46:22 2019) Epoch:  39  Training Cost:  0.03318861822672353 Training Accuracy:  0.9868686789214008\n",
      "(Sat Aug 31 19:46:31 2019) Epoch:  39  Test Cost:  0.059940969380973416 Test Accuracy:  0.9818181720646945 \n",
      "\n",
      "(Sat Aug 31 19:51:28 2019) Epoch:  40  Training Cost:  0.5009382953396951 Training Accuracy:  0.8878787786069543\n",
      "(Sat Aug 31 19:51:37 2019) Epoch:  40  Test Cost:  0.4968747676096179 Test Accuracy:  0.8818181698972528 \n",
      "\n",
      "(Sat Aug 31 19:56:34 2019) Epoch:  41  Training Cost:  0.1952097610349419 Training Accuracy:  0.9407407310273912\n",
      "(Sat Aug 31 19:56:43 2019) Epoch:  41  Test Cost:  0.13792361632328143 Test Accuracy:  0.9575757492672313 \n",
      "\n",
      "(Sat Aug 31 20:01:40 2019) Epoch:  42  Training Cost:  0.18449995415332265 Training Accuracy:  0.9565656468121693\n",
      "(Sat Aug 31 20:01:49 2019) Epoch:  42  Test Cost:  0.12467705413953147 Test Accuracy:  0.9575757492672313 \n",
      "\n",
      "(Sat Aug 31 20:06:46 2019) Epoch:  43  Training Cost:  0.304820048984968 Training Accuracy:  0.9383838285099376\n",
      "(Sat Aug 31 20:06:55 2019) Epoch:  43  Test Cost:  0.31029825543307443 Test Accuracy:  0.9424242377281189 \n",
      "\n",
      "(Sat Aug 31 20:11:52 2019) Epoch:  44  Training Cost:  0.20092121406336022 Training Accuracy:  0.9367003218092099\n",
      "(Sat Aug 31 20:12:01 2019) Epoch:  44  Test Cost:  0.2564124556427652 Test Accuracy:  0.9333333210511641 \n",
      "\n",
      "(Sat Aug 31 20:16:57 2019) Epoch:  45  Training Cost:  0.3857115111219687 Training Accuracy:  0.9148148051416031\n",
      "(Sat Aug 31 20:17:06 2019) Epoch:  45  Test Cost:  0.4435991683805531 Test Accuracy:  0.9181818095120516 \n",
      "\n",
      "(Sat Aug 31 20:21:54 2019) Epoch:  46  Training Cost:  0.047046448817788834 Training Accuracy:  0.9835016757550866\n",
      "(Sat Aug 31 20:22:01 2019) Epoch:  46  Test Cost:  0.04001736241678538 Test Accuracy:  0.9909090833230452 \n",
      "\n",
      "(Sat Aug 31 20:26:25 2019) Epoch:  47  Training Cost:  0.26227304093643194 Training Accuracy:  0.9464646337008236\n",
      "(Sat Aug 31 20:26:32 2019) Epoch:  47  Test Cost:  0.293469319797375 Test Accuracy:  0.9454545324498956 \n",
      "\n",
      "(Sat Aug 31 20:31:00 2019) Epoch:  48  Training Cost:  0.034705891518560655 Training Accuracy:  0.9905723851136486\n",
      "(Sat Aug 31 20:31:08 2019) Epoch:  48  Test Cost:  0.028400857863413297 Test Accuracy:  0.9878787831826643 \n",
      "\n",
      "(Sat Aug 31 20:36:05 2019) Epoch:  49  Training Cost:  0.09159976699962187 Training Accuracy:  0.9680134557714366\n",
      "(Sat Aug 31 20:36:14 2019) Epoch:  49  Test Cost:  0.045668902193111455 Test Accuracy:  0.9818181666460905 \n",
      "\n",
      "(Sat Aug 31 20:41:11 2019) Epoch:  50  Training Cost:  0.1973897271808425 Training Accuracy:  0.9367003278298811\n",
      "(Sat Aug 31 20:41:20 2019) Epoch:  50  Test Cost:  0.1251850656487725 Test Accuracy:  0.9575757438486273 \n",
      "\n",
      "(Sat Aug 31 20:46:03 2019) Epoch:  51  Training Cost:  0.14290527419852694 Training Accuracy:  0.9565656462101021\n",
      "(Sat Aug 31 20:46:10 2019) Epoch:  51  Test Cost:  0.1824669307521121 Test Accuracy:  0.966666655106978 \n",
      "\n",
      "(Sat Aug 31 20:50:35 2019) Epoch:  52  Training Cost:  0.10084753583317371 Training Accuracy:  0.9686868600171021\n",
      "(Sat Aug 31 20:50:42 2019) Epoch:  52  Test Cost:  0.04829507931241427 Test Accuracy:  0.9818181666460905 \n",
      "\n",
      "(Sat Aug 31 20:55:16 2019) Epoch:  53  Training Cost:  0.05964083346219867 Training Accuracy:  0.9791245701337101\n",
      "(Sat Aug 31 20:55:25 2019) Epoch:  53  Test Cost:  0.04385566623055969 Test Accuracy:  0.9909090833230452 \n",
      "\n",
      "(Sat Aug 31 21:00:22 2019) Epoch:  54  Training Cost:  0.08907716187580016 Training Accuracy:  0.9764309657944573\n",
      "(Sat Aug 31 21:00:31 2019) Epoch:  54  Test Cost:  0.07645713299544613 Test Accuracy:  0.9818181720646945 \n",
      "\n",
      "(Sat Aug 31 21:05:28 2019) Epoch:  55  Training Cost:  0.057710632637382436 Training Accuracy:  0.9841750751842152\n",
      "(Sat Aug 31 21:05:37 2019) Epoch:  55  Test Cost:  0.06782999482344497 Test Accuracy:  0.9727272553877397 \n",
      "\n",
      "(Sat Aug 31 21:10:33 2019) Epoch:  56  Training Cost:  0.029492915542137742 Training Accuracy:  0.9892255826429888\n",
      "(Sat Aug 31 21:10:42 2019) Epoch:  56  Test Cost:  0.03347306727664545 Test Accuracy:  0.9939393888820302 \n",
      "\n",
      "(Sat Aug 31 21:15:38 2019) Epoch:  57  Training Cost:  0.16914140548319803 Training Accuracy:  0.9545454443103135\n",
      "(Sat Aug 31 21:15:47 2019) Epoch:  57  Test Cost:  0.1581074166407978 Test Accuracy:  0.9606060494076122 \n",
      "\n",
      "(Sat Aug 31 21:20:45 2019) Epoch:  58  Training Cost:  0.05268633137695195 Training Accuracy:  0.9858585788746073\n",
      "(Sat Aug 31 21:20:54 2019) Epoch:  58  Test Cost:  0.0663268488900609 Test Accuracy:  0.9878787777640603 \n",
      "\n",
      "(Sat Aug 31 21:25:52 2019) Epoch:  59  Training Cost:  0.13670137235624338 Training Accuracy:  0.9670033545205088\n",
      "(Sat Aug 31 21:26:01 2019) Epoch:  59  Test Cost:  0.11319578481330113 Test Accuracy:  0.9727272608063438 \n",
      "\n",
      "(Sat Aug 31 21:30:58 2019) Epoch:  60  Training Cost:  0.12170888286294425 Training Accuracy:  0.9680134557714366\n",
      "(Sat Aug 31 21:31:07 2019) Epoch:  60  Test Cost:  0.10001789934044196 Test Accuracy:  0.9818181720646945 \n",
      "\n",
      "(Sat Aug 31 21:36:04 2019) Epoch:  61  Training Cost:  0.5901182836128606 Training Accuracy:  0.8784511691392071\n",
      "(Sat Aug 31 21:36:13 2019) Epoch:  61  Test Cost:  0.5194912972775373 Test Accuracy:  0.8696969584985212 \n",
      "\n",
      "(Sat Aug 31 21:41:09 2019) Epoch:  62  Training Cost:  0.07931144169160322 Training Accuracy:  0.9841750763883494\n",
      "(Sat Aug 31 21:41:18 2019) Epoch:  62  Test Cost:  0.09402532790872184 Test Accuracy:  0.9727272608063438 \n",
      "\n",
      "(Sat Aug 31 21:46:15 2019) Epoch:  63  Training Cost:  0.05677947701277468 Training Accuracy:  0.986195277686071\n",
      "(Sat Aug 31 21:46:24 2019) Epoch:  63  Test Cost:  0.042080859468445524 Test Accuracy:  0.9909090833230452 \n",
      "\n",
      "(Sat Aug 31 21:51:22 2019) Epoch:  64  Training Cost:  0.08637645007790222 Training Accuracy:  0.9740740632770037\n",
      "(Sat Aug 31 21:51:31 2019) Epoch:  64  Test Cost:  0.07208096320656213 Test Accuracy:  0.9787878665057096 \n",
      "\n",
      "(Sat Aug 31 21:56:28 2019) Epoch:  65  Training Cost:  0.09882026631712139 Training Accuracy:  0.9703703606971587\n",
      "(Sat Aug 31 21:56:37 2019) Epoch:  65  Test Cost:  0.14233202393039723 Test Accuracy:  0.966666655106978 \n",
      "\n",
      "(Sat Aug 31 22:01:34 2019) Epoch:  66  Training Cost:  0.006288371112532332 Training Accuracy:  0.9973063956607472\n",
      "(Sat Aug 31 22:01:43 2019) Epoch:  66  Test Cost:  0.016984992630816785 Test Accuracy:  0.9878787831826643 \n",
      "\n",
      "(Sat Aug 31 22:06:41 2019) Epoch:  67  Training Cost:  0.18521457907803723 Training Accuracy:  0.9569023444194986\n",
      "(Sat Aug 31 22:06:51 2019) Epoch:  67  Test Cost:  0.16785589266907086 Test Accuracy:  0.9606060494076122 \n",
      "\n",
      "(Sat Aug 31 22:11:47 2019) Epoch:  68  Training Cost:  0.15793479103351016 Training Accuracy:  0.9612794488367408\n",
      "(Sat Aug 31 22:11:56 2019) Epoch:  68  Test Cost:  0.0797144474001305 Test Accuracy:  0.9787878610871055 \n",
      "\n",
      "(Sat Aug 31 22:16:53 2019) Epoch:  69  Training Cost:  0.15394390679688918 Training Accuracy:  0.9579124456704265\n",
      "(Sat Aug 31 22:17:02 2019) Epoch:  69  Test Cost:  0.184521999548782 Test Accuracy:  0.9515151435678656 \n",
      "\n",
      "(Sat Aug 31 22:21:59 2019) Epoch:  70  Training Cost:  1.6322674546578917 Training Accuracy:  0.703030303873197\n",
      "(Sat Aug 31 22:22:08 2019) Epoch:  70  Test Cost:  1.6485834473913366 Test Accuracy:  0.7151515212925997 \n",
      "\n",
      "(Sat Aug 31 22:27:05 2019) Epoch:  71  Training Cost:  0.15501767626406174 Training Accuracy:  0.9552188443415093\n",
      "(Sat Aug 31 22:27:15 2019) Epoch:  71  Test Cost:  0.18299863306773742 Test Accuracy:  0.9666666496883739 \n",
      "\n",
      "(Sat Aug 31 22:32:12 2019) Epoch:  72  Training Cost:  0.3075714438231756 Training Accuracy:  0.9198653089879739\n",
      "(Sat Aug 31 22:32:21 2019) Epoch:  72  Test Cost:  0.29441324926235457 Test Accuracy:  0.9242424152114175 \n",
      "\n",
      "(Sat Aug 31 22:37:18 2019) Epoch:  73  Training Cost:  0.1819475655987238 Training Accuracy:  0.9508417375159987\n",
      "(Sat Aug 31 22:37:27 2019) Epoch:  73  Test Cost:  0.1880891343409365 Test Accuracy:  0.9424242268909108 \n",
      "\n",
      "(Sat Aug 31 22:42:24 2019) Epoch:  74  Training Cost:  0.08845593011004656 Training Accuracy:  0.9730639614240087\n",
      "(Sat Aug 31 22:42:33 2019) Epoch:  74  Test Cost:  0.09383342741057277 Test Accuracy:  0.9696969552473589 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Sat Aug 31 22:47:30 2019) Epoch:  75  Training Cost:  0.22744966235452488 Training Accuracy:  0.9632996543489322\n",
      "(Sat Aug 31 22:47:39 2019) Epoch:  75  Test Cost:  0.24045419693522324 Test Accuracy:  0.9575757384300232 \n",
      "\n",
      "(Sat Aug 31 22:52:35 2019) Epoch:  76  Training Cost:  0.6184659124530777 Training Accuracy:  0.8936026854948564\n",
      "(Sat Aug 31 22:52:44 2019) Epoch:  76  Test Cost:  0.6991803203286096 Test Accuracy:  0.9060606035319242 \n",
      "\n",
      "(Sat Aug 31 22:57:41 2019) Epoch:  77  Training Cost:  0.6683720805578761 Training Accuracy:  0.8249158209020441\n",
      "(Sat Aug 31 22:57:50 2019) Epoch:  77  Test Cost:  0.6572681529955431 Test Accuracy:  0.81818180734461 \n",
      "\n",
      "(Sat Aug 31 23:02:47 2019) Epoch:  78  Training Cost:  0.7517457162265224 Training Accuracy:  0.850841745583698\n",
      "(Sat Aug 31 23:02:56 2019) Epoch:  78  Test Cost:  0.8109432242133401 Test Accuracy:  0.851515141400424 \n",
      "\n",
      "(Sat Aug 31 23:07:53 2019) Epoch:  79  Training Cost:  0.06896409790204765 Training Accuracy:  0.9754208633393953\n",
      "(Sat Aug 31 23:08:02 2019) Epoch:  79  Test Cost:  0.12248176283372397 Test Accuracy:  0.9727272553877397 \n",
      "\n",
      "(Sat Aug 31 23:13:00 2019) Epoch:  80  Training Cost:  0.06166737491355778 Training Accuracy:  0.9797979689607716\n",
      "(Sat Aug 31 23:13:09 2019) Epoch:  80  Test Cost:  0.10046535585901108 Test Accuracy:  0.9696969606659629 \n",
      "\n",
      "(Sat Aug 31 23:18:06 2019) Epoch:  81  Training Cost:  0.6620929776079426 Training Accuracy:  0.8858585761050985\n",
      "(Sat Aug 31 23:18:15 2019) Epoch:  81  Test Cost:  0.8286606825210832 Test Accuracy:  0.8727272694761102 \n",
      "\n",
      "(Sat Aug 31 23:23:12 2019) Epoch:  82  Training Cost:  0.15325427727040017 Training Accuracy:  0.9451178354446335\n",
      "(Sat Aug 31 23:23:21 2019) Epoch:  82  Test Cost:  0.2009653379632668 Test Accuracy:  0.9545454437082465 \n",
      "\n",
      "(Sat Aug 31 23:28:18 2019) Epoch:  83  Training Cost:  0.13566298432333773 Training Accuracy:  0.9515151399554629\n",
      "(Sat Aug 31 23:28:28 2019) Epoch:  83  Test Cost:  0.1996969362212853 Test Accuracy:  0.9484848325902765 \n",
      "\n",
      "(Sat Aug 31 23:33:23 2019) Epoch:  84  Training Cost:  0.07493459734306647 Training Accuracy:  0.9767676652079881\n",
      "(Sat Aug 31 23:33:32 2019) Epoch:  84  Test Cost:  0.09970297261448154 Test Accuracy:  0.9696969552473589 \n",
      "\n",
      "(Sat Aug 31 23:38:29 2019) Epoch:  85  Training Cost:  0.14408127584215666 Training Accuracy:  0.9629629501188645\n",
      "(Sat Aug 31 23:38:38 2019) Epoch:  85  Test Cost:  0.13124548866868613 Test Accuracy:  0.9727272608063438 \n",
      "\n",
      "(Sat Aug 31 23:43:35 2019) Epoch:  86  Training Cost:  0.6777340073627655 Training Accuracy:  0.8754208689988262\n",
      "(Sat Aug 31 23:43:44 2019) Epoch:  86  Test Cost:  0.7382675558328629 Test Accuracy:  0.8727272694761102 \n",
      "\n",
      "(Sat Aug 31 23:48:41 2019) Epoch:  87  Training Cost:  1.0185025564648889 Training Accuracy:  0.7767676753227157\n",
      "(Sat Aug 31 23:48:50 2019) Epoch:  87  Test Cost:  1.0478200153871016 Test Accuracy:  0.7909090952439741 \n",
      "\n",
      "(Sat Aug 31 23:53:47 2019) Epoch:  88  Training Cost:  0.4044468217915056 Training Accuracy:  0.8898989841191456\n",
      "(Sat Aug 31 23:53:56 2019) Epoch:  88  Test Cost:  0.4612279297275977 Test Accuracy:  0.8878787755966187 \n",
      "\n",
      "(Sat Aug 31 23:58:52 2019) Epoch:  89  Training Cost:  0.6149493777475348 Training Accuracy:  0.9104377031326294\n",
      "(Sat Aug 31 23:59:01 2019) Epoch:  89  Test Cost:  0.6304600482637231 Test Accuracy:  0.8939393812959845 \n",
      "\n",
      "(Sun Sep  1 00:03:57 2019) Epoch:  90  Training Cost:  0.11293476233961451 Training Accuracy:  0.9723905625969472\n",
      "(Sun Sep  1 00:04:06 2019) Epoch:  90  Test Cost:  0.13873431818369267 Test Accuracy:  0.966666655106978 \n",
      "\n",
      "(Sun Sep  1 00:09:02 2019) Epoch:  91  Training Cost:  0.23981861973269855 Training Accuracy:  0.9622895512918029\n",
      "(Sun Sep  1 00:09:11 2019) Epoch:  91  Test Cost:  0.19053880699952555 Test Accuracy:  0.966666655106978 \n",
      "\n",
      "(Sun Sep  1 00:14:07 2019) Epoch:  92  Training Cost:  0.24972690957612498 Training Accuracy:  0.9313131209575769\n",
      "(Sun Sep  1 00:14:17 2019) Epoch:  92  Test Cost:  0.2725493068891493 Test Accuracy:  0.9424242214723066 \n",
      "\n",
      "(Sun Sep  1 00:19:13 2019) Epoch:  93  Training Cost:  0.41345145595448113 Training Accuracy:  0.9026935967532072\n",
      "(Sun Sep  1 00:19:22 2019) Epoch:  93  Test Cost:  0.44646095619960263 Test Accuracy:  0.8969696922735735 \n",
      "\n",
      "(Sun Sep  1 00:24:19 2019) Epoch:  94  Training Cost:  0.6878352251016733 Training Accuracy:  0.8468013417841208\n",
      "(Sun Sep  1 00:24:28 2019) Epoch:  94  Test Cost:  0.6877773566679521 Test Accuracy:  0.854545457796617 \n",
      "\n",
      "(Sun Sep  1 00:29:25 2019) Epoch:  95  Training Cost:  0.6359359687852739 Training Accuracy:  0.8720538664345789\n",
      "(Sun Sep  1 00:29:34 2019) Epoch:  95  Test Cost:  0.6325132142413746 Test Accuracy:  0.8575757579369978 \n",
      "\n",
      "(Sun Sep  1 00:34:32 2019) Epoch:  96  Training Cost:  0.15820789239733865 Training Accuracy:  0.9579124456704265\n",
      "(Sun Sep  1 00:34:41 2019) Epoch:  96  Test Cost:  0.12434245176105337 Test Accuracy:  0.9696969606659629 \n",
      "\n",
      "(Sun Sep  1 00:39:37 2019) Epoch:  97  Training Cost:  0.35434001434777834 Training Accuracy:  0.9404040304097262\n",
      "(Sun Sep  1 00:39:46 2019) Epoch:  97  Test Cost:  0.2886620982317254 Test Accuracy:  0.9515151435678656 \n",
      "\n",
      "(Sun Sep  1 00:44:42 2019) Epoch:  98  Training Cost:  0.5127404899444582 Training Accuracy:  0.8986531917494957\n",
      "(Sun Sep  1 00:44:52 2019) Epoch:  98  Test Cost:  0.4876554145223715 Test Accuracy:  0.9181818095120516 \n",
      "\n",
      "(Sun Sep  1 00:49:49 2019) Epoch:  99  Training Cost:  0.24524329631880978 Training Accuracy:  0.9289562208483918\n",
      "(Sun Sep  1 00:49:58 2019) Epoch:  99  Test Cost:  0.308220412328162 Test Accuracy:  0.9242424152114175 \n",
      "\n",
      "(Sun Sep  1 00:54:54 2019) Epoch:  100  Training Cost:  0.07758251961481298 Training Accuracy:  0.9784511676942459\n",
      "(Sun Sep  1 00:55:03 2019) Epoch:  100  Test Cost:  0.0871472096663307 Test Accuracy:  0.9787878773429177 \n",
      "\n",
      "******************root: 0.006060606060128923\n",
      "(Sun Sep  1 00:55:14 2019) Final Test Cost:  0.0871472096663307 Final Test Accuracy:  0.9787878773429177\n",
      "Variable_1:0 --> (3, 3, 1, 32)\n",
      "Variable_2:0 --> (32,)\n",
      "Variable_3:0 --> (3, 3, 32, 64)\n",
      "Variable_4:0 --> (64,)\n",
      "Variable_5:0 --> (3, 3, 64, 128)\n",
      "Variable_6:0 --> (128,)\n",
      "Variable_7:0 --> (14080, 1024)\n",
      "Variable_8:0 --> (1024,)\n",
      "rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0 --> (2048, 4096)\n",
      "rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0 --> (4096,)\n",
      "rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0 --> (2048, 4096)\n",
      "rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0 --> (4096,)\n",
      "Variable_9:0 --> (1024, 1024)\n",
      "Variable_10:0 --> (1024,)\n",
      "Variable_11:0 --> (1024, 15)\n",
      "Variable_12:0 --> (15,)\n",
      "----------------------------------------------------------------\n",
      "------------------total parameters 32361999 -----------------------\n",
      "----------------------------------------------------------------\n",
      "**********(Sun Sep  1 00:55:14 2019) Train and Test NN End **********\n",
      "\n",
      "**********(Sun Sep  1 00:55:14 2019) Train and Test NN End **********\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_x = X_train\n",
    "train_y = y_train\n",
    "\n",
    "train_sample = len(train_x)\n",
    "print(\"train sample:\", train_sample)\n",
    "\n",
    "# test_x = datasets[~split]\n",
    "# test_y = labels[~split]\n",
    "\n",
    "test_x = X_test\n",
    "test_y = y_test\n",
    "\n",
    "test_sample = len(test_x)\n",
    "print(\"test sample:\", test_sample)\n",
    "\n",
    "print(\"**********(\" + time.asctime(time.localtime(time.time())) + \") Load and Split dataset End **********\\n\")\n",
    "\n",
    "print(\n",
    "    \"**********(\" + time.asctime(time.localtime(time.time())) + \") Define parameters and functions Begin: **********\\n\")\n",
    "\n",
    "# input parameter\n",
    "input_channel_num = 1\n",
    "\n",
    "input_height = 10\n",
    "input_width = 11\n",
    "\n",
    "n_labels = 15\n",
    "\n",
    "# training parameter\n",
    "lambda_loss_amount = 0.0001\n",
    "training_epochs = 100\n",
    "batch_size = 30\n",
    "batch_num_per_epoch = train_x.shape[0] // batch_size\n",
    "\n",
    "accuracy_batch_size = 30\n",
    "train_accuracy_batch_num = train_x.shape[0] // accuracy_batch_size\n",
    "test_accuracy_batch_num = test_x.shape[0] // accuracy_batch_size\n",
    "\n",
    "# kernel parameter\n",
    "kernel_height_1st = 3\n",
    "kernel_width_1st = 3\n",
    "\n",
    "kernel_height_2nd = 3\n",
    "kernel_width_2nd = 3\n",
    "\n",
    "kernel_height_3rd = 3\n",
    "kernel_width_3rd = 3\n",
    "\n",
    "kernel_stride = 1\n",
    "conv_channel_num = 32\n",
    "# pooling parameter\n",
    "pooling_height = 2\n",
    "pooling_width = 2\n",
    "\n",
    "pooling_stride = 2\n",
    "\n",
    "# algorithn parameter\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "initial_learning_rate = 0.001  # 初始学习率  # 初始学习率太大了，容易造成结果波动，但是不知道会不会鲁棒性好？\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(initial_learning_rate,\n",
    "                                           global_step=global_step,\n",
    "                                           decay_steps=10, decay_rate=0.9)\n",
    "\n",
    "\n",
    "# learning_rate = 1e-4\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv2d(x, W, kernel_stride):\n",
    "    # API: must strides[0]=strides[4]=1\n",
    "    return tf.nn.conv2d(x, W, strides=[1, kernel_stride, kernel_stride, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def apply_conv2d(x, filter_height, filter_width, in_channels, out_channels, kernel_stride):\n",
    "    weight = weight_variable([filter_height, filter_width, in_channels, out_channels])\n",
    "    bias = bias_variable([out_channels])  # each feature map shares the same weight and bias\n",
    "    return tf.nn.elu(tf.add(conv2d(x, weight, kernel_stride), bias))\n",
    "\n",
    "\n",
    "def apply_max_pooling(x, pooling_height, pooling_width, pooling_stride):\n",
    "    # API: must ksize[0]=ksize[4]=1, strides[0]=strides[4]=1\n",
    "    return tf.nn.max_pool(x, ksize=[1, pooling_height, pooling_width, 1],\n",
    "                          strides=[1, pooling_stride, pooling_stride, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def apply_fully_connect(x, x_size, fc_size):\n",
    "    fc_weight = weight_variable([x_size, fc_size])\n",
    "    fc_bias = bias_variable([fc_size])\n",
    "    return tf.nn.elu(tf.add(tf.matmul(x, fc_weight), fc_bias))\n",
    "\n",
    "\n",
    "def apply_readout(x, x_size, readout_size):\n",
    "    readout_weight = weight_variable([x_size, readout_size])\n",
    "    readout_bias = bias_variable([readout_size])\n",
    "    return tf.add(tf.matmul(x, readout_weight), readout_bias)\n",
    "\n",
    "\n",
    "print(\"**********(\" + time.asctime(time.localtime(time.time())) + \") Define parameters and functions End **********\\n\")\n",
    "\n",
    "print(\"**********(\" + time.asctime(time.localtime(time.time())) + \") Define NN structure Begin: **********\\n\")\n",
    "\n",
    "# input placeholder\n",
    "X = tf.placeholder(tf.float32, shape=[None, input_height, input_width, input_channel_num], name='X')\n",
    "Y = tf.placeholder(tf.float32, shape=[None, n_labels], name='Y')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "\n",
    "# first CNN layer\n",
    "conv_1 = apply_conv2d(X, kernel_height_1st, kernel_width_1st, input_channel_num, conv_channel_num, kernel_stride)\n",
    "# pool_1 = apply_max_pooling(conv_1, pooling_height, pooling_width, pooling_stride)\n",
    "print(conv_1.shape)\n",
    "# second CNN layer\n",
    "conv_2 = apply_conv2d(conv_1, kernel_height_2nd, kernel_width_2nd, conv_channel_num, conv_channel_num * 2,\n",
    "                      kernel_stride)\n",
    "# pool_2 = apply_max_pooling(conv_2, pooling_height, pooling_width, pooling_stride)\n",
    "print(conv_2.shape)\n",
    "# third CNN layer\n",
    "conv_3 = apply_conv2d(conv_2, kernel_height_3rd, kernel_width_3rd, conv_channel_num * 2, conv_channel_num * 4,\n",
    "                      kernel_stride)\n",
    "# fully connected layer\n",
    "print(conv_3.shape)\n",
    "shape = conv_3.get_shape().as_list()\n",
    "\n",
    "pool_2_flat = tf.reshape(conv_3, [-1, shape[1] * shape[2] * shape[3]])\n",
    "fc = apply_fully_connect(pool_2_flat, shape[1] * shape[2] * shape[3], fc_size)\n",
    "\n",
    "# dropout regularizer\n",
    "# Dropout (to reduce overfitting; useful when training very large neural network)\n",
    "# We will turn on dropout during training & turn off during testing\n",
    "\n",
    "fc_drop = tf.nn.dropout(fc, keep_prob)\n",
    "\n",
    "# fc_drop size [batch_size*window_size, fc_size]\n",
    "# lstm_in size [batch_size, window_size, fc_size]\n",
    "lstm_in = tf.reshape(fc_drop, [-1, window_size, fc_size])\n",
    "\n",
    "cells = []\n",
    "for _ in range(n_lstm_layers):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(n_fc_in, forget_bias=1.0, state_is_tuple=True)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "    cells.append(cell)\n",
    "lstm_cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "\n",
    "output, states = tf.nn.dynamic_rnn(lstm_cell, lstm_in, initial_state=init_state, time_major=False)\n",
    "\n",
    "output = tf.unstack(tf.transpose(output, [1, 0, 2]), name='lstm_out')\n",
    "rnn_output = output[-1]\n",
    "\n",
    "shape_rnn_out = rnn_output.get_shape().as_list()\n",
    "# fc_out ==> [batch_size, n_fc_out]\n",
    "fc_out = apply_fully_connect(rnn_output, shape_rnn_out[1], n_fc_out)\n",
    "\n",
    "# keep_prob = tf.placeholder(tf.float32)\n",
    "fc_drop = tf.nn.dropout(fc_out, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "y_ = apply_readout(fc_drop, shape_rnn_out[1], n_labels)\n",
    "y_pred = tf.argmax(tf.nn.softmax(y_), 1, name=\"y_pred\")\n",
    "y_posi = tf.nn.softmax(y_, name=\"y_posi\")\n",
    "\n",
    "# l2 regularization\n",
    "l2 = lambda_loss_amount * sum(\n",
    "    tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    ")\n",
    "\n",
    "if enable_penalty:\n",
    "    # cross entropy cost function\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=Y) + l2, name='loss')\n",
    "else:\n",
    "    # cross entropy cost function\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=Y), name='loss')\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "add_global = global_step.assign_add(1)\n",
    "\n",
    "# get correctly predicted object and accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(y_), 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print(\"**********(\" + time.asctime(time.localtime(time.time())) + \") Define NN structure End **********\\n\")\n",
    "\n",
    "print(\"**********(\" + time.asctime(time.localtime(time.time())) + \") Train and Test NN Begin: **********\\n\")\n",
    "# run\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config=config) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    train_accuracy_save = np.zeros(shape=[0], dtype=float)\n",
    "    test_accuracy_save = np.zeros(shape=[0], dtype=float)\n",
    "    test_loss_save = np.zeros(shape=[0], dtype=float)\n",
    "    train_loss_save = np.zeros(shape=[0], dtype=float)\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.zeros(shape=[0], dtype=float)\n",
    "        for b in range(batch_num_per_epoch):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :, :]\n",
    "            batch_x = batch_x.reshape(len(batch_x) * window_size, 10, 11, 1)\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, cost],\n",
    "                               feed_dict={X: batch_x, Y: batch_y, keep_prob: 1 - dropout_prob, phase_train: True})\n",
    "            cost_history = np.append(cost_history, c)\n",
    "        if (epoch % 1 == 0):\n",
    "            train_accuracy = np.zeros(shape=[0], dtype=float)\n",
    "            test_accuracy = np.zeros(shape=[0], dtype=float)\n",
    "            test_loss = np.zeros(shape=[0], dtype=float)\n",
    "            train_loss = np.zeros(shape=[0], dtype=float)\n",
    "            for i in range(train_accuracy_batch_num):\n",
    "                offset = (i * accuracy_batch_size) % (train_y.shape[0] - accuracy_batch_size)\n",
    "                train_batch_x = train_x[offset:(offset + accuracy_batch_size), :, :, :, :]\n",
    "                train_batch_x = train_batch_x.reshape(len(train_batch_x) * window_size, 10, 11, 1)\n",
    "                train_batch_y = train_y[offset:(offset + accuracy_batch_size), :]\n",
    "\n",
    "                train_a, train_c = session.run([accuracy, cost],\n",
    "                                               feed_dict={X: train_batch_x, Y: train_batch_y, keep_prob: 1.0,\n",
    "                                                          phase_train: False})\n",
    "\n",
    "                train_loss = np.append(train_loss, train_c)\n",
    "                train_accuracy = np.append(train_accuracy, train_a)\n",
    "            print(\"(\" + time.asctime(time.localtime(time.time())) + \") Epoch: \", epoch + 1, \" Training Cost: \",\n",
    "                  np.mean(train_loss), \"Training Accuracy: \", np.mean(train_accuracy))\n",
    "            train_accuracy_save = np.append(train_accuracy_save, np.mean(train_accuracy))\n",
    "            train_loss_save = np.append(train_loss_save, np.mean(train_loss))\n",
    "            for j in range(test_accuracy_batch_num):\n",
    "                offset = (j * accuracy_batch_size) % (test_y.shape[0] - accuracy_batch_size)\n",
    "                test_batch_x = test_x[offset:(offset + accuracy_batch_size), :, :, :, :]\n",
    "                test_batch_x = test_batch_x.reshape(len(test_batch_x) * window_size, 10, 11, 1)\n",
    "                test_batch_y = test_y[offset:(offset + accuracy_batch_size), :]\n",
    "\n",
    "                test_a, test_c = session.run([accuracy, cost],\n",
    "                                             feed_dict={X: test_batch_x, Y: test_batch_y, keep_prob: 1.0,\n",
    "                                                        phase_train: False})\n",
    "\n",
    "                test_accuracy = np.append(test_accuracy, test_a)\n",
    "                test_loss = np.append(test_loss, test_c)\n",
    "\n",
    "            print(\"(\" + time.asctime(time.localtime(time.time())) + \") Epoch: \", epoch + 1, \" Test Cost: \",\n",
    "                  np.mean(test_loss), \"Test Accuracy: \", np.mean(test_accuracy), \"\\n\")\n",
    "            test_accuracy_save = np.append(test_accuracy_save, np.mean(test_accuracy))\n",
    "            test_loss_save = np.append(test_loss_save, np.mean(test_loss))\n",
    "    test_accuracy = np.zeros(shape=[0], dtype=float)\n",
    "    test_loss = np.zeros(shape=[0], dtype=float)\n",
    "    test_pred = np.zeros(shape=[0], dtype=float)\n",
    "    test_true = np.zeros(shape=[0, 15], dtype=float)\n",
    "    test_posi = np.zeros(shape=[0, 15], dtype=float)\n",
    "    for k in range(test_accuracy_batch_num):\n",
    "        offset = (k * accuracy_batch_size) % (test_y.shape[0] - accuracy_batch_size)\n",
    "        test_batch_x = test_x[offset:(offset + accuracy_batch_size), :, :, :, :]\n",
    "        test_batch_x = test_batch_x.reshape(len(test_batch_x) * window_size, 10, 11, 1)\n",
    "        test_batch_y = test_y[offset:(offset + accuracy_batch_size), :]\n",
    "\n",
    "        test_a, test_c, test_p, test_r = session.run([accuracy, cost, y_pred, y_posi],\n",
    "                                                     feed_dict={X: test_batch_x, Y: test_batch_y, keep_prob: 1.0,\n",
    "                                                                phase_train: False})\n",
    "        test_t = test_batch_y  # one-hot 格式\n",
    "\n",
    "        test_accuracy = np.append(test_accuracy, test_a)\n",
    "        test_loss = np.append(test_loss, test_c)\n",
    "        test_pred = np.append(test_pred, test_p)\n",
    "        test_true = np.vstack([test_true, test_t])\n",
    "        test_posi = np.vstack([test_posi, test_r])\n",
    "#     test_true = tf.argmax(test_true, 1)\n",
    "    test_pred_1_hot = np.asarray(pd.get_dummies(test_pred), dtype = np.int8)\n",
    "    test_true_list = tf.argmax(test_true, 1).eval()\n",
    "    # print(test_pred.shape)\n",
    "    # print(test_pred)\n",
    "    # print(test_true.shape)\n",
    "    # print(test_true)\n",
    "#     test_true = tf.argmax(test_true, 1)\n",
    "#     test_true_np = test_true.eval()\n",
    "#     # print(test_true_np)\n",
    "#     test_true = test_true_np\n",
    "#     test_true_list = test_true_np\n",
    "    test_pred_1_hot = test_pred\n",
    "    # print(test_pred.shape)\n",
    "    # print(test_pred)\n",
    "    \n",
    "    y_score = test_posi\n",
    "    n_classes = test_true.shape[1]\n",
    "    test_size = len(y_score)\n",
    "    total_comparisions = test_size * n_classes\n",
    "\n",
    "    far, frr = prepare_graph_far_frr(test_true, y_score, total_comparisions - test_size, test_size)\n",
    "    root = brentq(lambda x: x - interpolate.interp1d(far, frr)(x), min(far), 1.0)\n",
    "\n",
    "    # recall\n",
    "#     test_recall = recall_score(test_true, test_pred, average=None)\n",
    "#     # precision\n",
    "#     test_precision = precision_score(test_true, test_pred_1_hot, average=None)\n",
    "#     # f1 score\n",
    "#     test_f1 = f1_score(test_true, test_pred_1_hot, average=None)\n",
    "    # auc\n",
    "    # test_auc = roc_auc_score(test_true, test_pred_1_hot, average=None)\n",
    "    # confusion matrix\n",
    "    # \tconfusion_matrix = confusion_matrix(test_true_list, test_pred)\n",
    "\n",
    "    # plot_confusion_matrix(test_true, test_pred,[0,1,2,3,4,5,6,7,8])\n",
    "\n",
    "#     print(\"********************recall:\", test_recall)\n",
    "#     print(\"*****************precision:\", test_precision)\n",
    "#     # print(\"******************test_auc:\", test_auc)\n",
    "#     print(\"******************f1_score:\", test_f1)\n",
    "    print(\"******************root:\", root)\n",
    "    # \tprint(\"**********confusion_matrix:\\n\", confusion_matrix)\n",
    "\n",
    "    print(\"(\" + time.asctime(time.localtime(time.time())) + \") Final Test Cost: \", np.mean(test_loss),\n",
    "          \"Final Test Accuracy: \", np.mean(test_accuracy))\n",
    "    # save result\n",
    "\n",
    "    result = pd.DataFrame(\n",
    "        {'epoch': range(1, epoch + 2), \"train_accuracy\": train_accuracy_save, \"test_accuracy\": test_accuracy_save,\n",
    "         \"train_loss\": train_loss_save, \"test_loss\": test_loss_save})\n",
    "    ins = pd.DataFrame({'conv_1': conv_1_shape, 'pool_1': pool_1_shape, 'conv_2': conv_2_shape, 'pool_2': pool_2_shape,\n",
    "                        'conv_3': conv_3_shape, 'pool_3': pool_3_shape, 'conv_4': conv_4_shape, 'pool_3': pool_3_shape,\n",
    "                        'fc': fc_size, 'accuracy': np.mean(test_accuracy), 'keep_prob': 1 - dropout_prob,\n",
    "                        \"calibration\": calibration, 'sliding_window': window_size, \"epoch\": epoch + 1,\n",
    "                        \"norm\": norm_type, \"learning_rate\": learning_rate, \"regularization\": regularization_method,\n",
    "                        \"train_sample\": train_sample, \"test_sample\": test_sample}, index=[0])\n",
    "    # summary = pd.DataFrame({'class':one_hot_labels, 'recall':test_recall, 'precision':test_precision, 'f1_score':test_f1, 'roc_auc':test_auc})\n",
    "\n",
    "    writer = pd.ExcelWriter(\"./result/\" + \"output_file_p_15_32\" + \".xlsx\")\n",
    "    ins.to_excel(writer, 'condition', index = False)\n",
    "    result.to_excel(writer, 'result', index = False)\n",
    "    # summary.to_excel(writer, 'summary', index=False)\n",
    "    # fpr, tpr, auc\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    i = 0\n",
    "    # for key in one_hot_labels:\n",
    "    # \tfpr[key], tpr[key], _ = roc_curve(test_true[:, i], test_posi[:, i])\n",
    "    # \troc_auc[key] = auc(fpr[key], tpr[key])\n",
    "    # \troc = pd.DataFrame({\"fpr\":fpr[key], \"tpr\":tpr[key], \"roc_auc\":roc_auc[key]})\n",
    "    # \troc.to_excel(writer, key, index=False)\n",
    "    # \ti += 1\n",
    "    writer.save()\n",
    "    model_dict = {}\n",
    "    parameter_count = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        print(variable.name, \"-->\", variable.get_shape())\n",
    "        count = 1\n",
    "        for dim in variable.get_shape().as_list():\n",
    "            count = count * dim\n",
    "        parameter_count = parameter_count + count\n",
    "        model_dict[variable.name] = session.run(variable)\n",
    "    # \tsio.savemat(str(parameter_count)+\".mat\", model_dict)\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"------------------total parameters\", parameter_count, \"-----------------------\")\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "\n",
    "    print(\"**********(\" + time.asctime(time.localtime(time.time())) + \") Train and Test NN End **********\\n\")\n",
    "# \tpickle.dump(confusion_matrix, fp)\n",
    "# # save model\n",
    "# \tsaver.save(session, './results_p_model_a_b/model_p_a_b.ckpt')\n",
    "\n",
    "\n",
    "print(\"**********(\" + time.asctime(time.localtime(time.time())) + \") Train and Test NN End **********\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
